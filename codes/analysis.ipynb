{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext, Row\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "from pyspark.sql.functions import stddev_pop, avg, broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcessUsers(data):\n",
    "\ttry:\n",
    "\t\troot = ET.fromstring(data)\n",
    "\t        return [int(root.attrib['Id']), int(root.attrib['Reputation']), int(root.attrib['Views']), int(root.attrib['UpVotes']), int(root.attrib['DownVotes']), int(root.attrib['Age'])]\n",
    "\texcept:\n",
    "\t\tprint(\"Ignoring record\")\n",
    "\n",
    "\n",
    "\n",
    "def preProcessBadges(data):\n",
    "\ttry:\n",
    "\t\troot = ET.fromstring(data)\n",
    "\t        return [int(root.attrib['UserId']), root.attrib['Name']]\n",
    "\texcept:\n",
    "\t\tprint(\"Ignoring record\")\n",
    "\n",
    "def preProcessPosts(data):\n",
    "\ttry:\n",
    "\t\troot = ET.fromstring(data)\n",
    "\t        return [int(root.attrib['Id']), int(root.attrib['PostTypeId']), root.attrib['OwnerUserId']]\n",
    "\texcept:\n",
    "\t\tprint(\"Ignoring record\")\n",
    "\n",
    "def preProcessComments(data):\n",
    "\ttry:\n",
    "\t\troot = ET.fromstring(data)\n",
    "\t        return [int(root.attrib['Id']), int(root.attrib['UserId'])]\n",
    "\texcept:\n",
    "\t\tprint(\"Ignoring record\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = sc.textFile(\"stackOverflowUserOutput\", 10)\n",
    "users = sc.textFile(\"/data/stackoverflow/Users\", 25)\n",
    "posts = sc.textFile(\"/data/stackoverflow/Posts\", 25)\n",
    "comments = sc.textFile(\"/data/stackoverflow/Comments\", 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessOutput(data):\n",
    "\tx = data.split(\",\")\n",
    "\treturn (int(x[0].replace(\"((\",\"\")), float(x[1].replace(\")\",\"\")), int(x[2].replace(\")\",\"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputDF = output.map(preprocessOutput).filter(lambda x: x is not None).toDF(['Cluster', 'Score', 'UserId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "usersDF = users.map(preProcessUsers).filter(lambda x: x is not None).toDF(['UserId', 'Reputation', 'Views', 'UpVotes', 'DownVotes', 'Age'])\n",
    "\n",
    "postsData = posts.map(preProcessPosts).filter(lambda x: x is not None).toDF(['Id', 'PostTypeId', 'UserId'])\n",
    "filteredPostData = postsData.filter(\"PostTypeId = 2\")\n",
    "postsDF = filteredPostData.groupBy(\"UserId\").count().withColumnRenamed(\"count\", \"post_count\")\n",
    "\n",
    "commentsData = comments.map(preProcessComments).filter(lambda x: x is not None).toDF(['Id', 'UserId'])\n",
    "commentsDF = commentsData.groupBy(\"UserId\").count().withColumnRenamed(\"count\", \"comment_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "badges = sc.textFile(\"/data/stackoverflow/Badges\", 25)\n",
    "badgesData = badges.map(preProcessBadges).filter(lambda x: x is not None)\n",
    "badgesDF = badgesData.groupByKey().map(lambda (key, val): (key, list(val))).toDF(['UserId', 'Badges'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalData = outputDF.join(usersDF, [\"UserId\"], \"left_outer\").join(postsDF, [\"UserId\"], \"left_outer\").join(commentsDF, [\"UserId\"], \"left_outer\").join(badgesDF, [\"UserId\"], \"left_outer\").fillna(0).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------------------+----------+-----+-------+---------+---+----------+-------------+--------------------+\n",
      "|UserId|Cluster|             Score|Reputation|Views|UpVotes|DownVotes|Age|post_count|comment_count|              Badges|\n",
      "+------+-------+------------------+----------+-----+-------+---------+---+----------+-------------+--------------------+\n",
      "|   231|      3|10.635831749543929|      3781|  175|    167|        3| 32|        39|           22|[Nice Question, P...|\n",
      "|   431|      3| 10.26390043907491|      2733|  319|    233|        1| 34|        82|           53|[Famous Question,...|\n",
      "|   631|      3| 20.74530278358818|      7050|  551|    690|        3| 36|        81|           86|[Quorum, Good Ans...|\n",
      "|  1031|      3| 9.272470473670413|      1771|  212|    100|        4| 43|         8|            9|[Popular Question...|\n",
      "|  1231|      3|17.605089046029455|      3738|  537|    427|       20| 38|        48|          137|[Popular Question...|\n",
      "|  1631|      3|13.642011766852974|      2159|  172|    396|        6| 24|        41|           27|[Necromancer, Nic...|\n",
      "|  1831|      2|146.08447874236052|     52730| 7067|   2705|       95| 34|      1143|         2071|[Notable Question...|\n",
      "|  2031|      1| 66.84113960173256|      3331|  636|    533|        5| 94|        54|           92|[Popular Question...|\n",
      "|  2231|      3|14.494636239643537|      2815|  129|    156|        5| 41|        65|           40|[Popular Question...|\n",
      "|  2831|      3| 18.04533282119079|      2768|  636|    508|       36| 34|        52|           77|[Popular Question...|\n",
      "|  3031|      3| 6.194340701820083|       417|   26|    115|        0| 37|         9|            3|[Student, Scholar...|\n",
      "|  3431|      3|5.0319800055474015|       466|  189|    100|        4| 35|        11|           74|[Popular Question...|\n",
      "|  3831|      3|12.545350197278815|      2522|  334|    398|       28| 26|        60|          138|[Popular Question...|\n",
      "|  4231|      3|10.131597471298008|      2661|  143|     85|        0| 39|        27|           14|[Popular Question...|\n",
      "|  4431|      3| 5.308073592455908|      1150|   68|     92|        3| 35|        22|            8|[Notable Question...|\n",
      "|  4831|      3| 7.587453362516933|       155|   58|     47|        2| 29|         3|            4|[Popular Question...|\n",
      "|  5431|      1| 8.844027168844987|       141|    9|      1|        0| 56|         1|            0|[Self-Learner, Su...|\n",
      "|  6231|      3| 4.327614778384792|       111|   20|      4|        0| 28|         5|            1|[Popular Question...|\n",
      "|  7031|      1|  8.51846664108581|      1909|   38|      5|        0| 41|         5|            0|[Popular Question...|\n",
      "|  7231|      2|22.986857744524592|     15151|  681|   2133|      327| 52|       466|          670|[Fanatic, perl, C...|\n",
      "+------+-------+------------------+----------+-----+-------+---------+---+----------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finalData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalData.write.format('com.databricks.spark.csv').save('withBadgeCSV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessOutputMlib(data):\n",
    "\tx = data.split(\",\")\n",
    "\treturn (int(x[0].replace(\"(\",\"\")), int(x[1].replace(\")\",\"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputMlib = sc.textFile(\"ClustersMlib\", 10)\n",
    "outputMlibDF = outputMlib.map(preprocessOutputMlib).filter(lambda x: x is not None).toDF(['Cluster', 'UserId'])\n",
    "outputMlibDF.write.format('com.databricks.spark.csv').save('mlibCSV')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
